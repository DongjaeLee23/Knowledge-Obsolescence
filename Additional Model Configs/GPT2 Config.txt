gpt2_config = {
    "data_param": {
        "dataset": "time_sorted",
        "max_data_size": -1,
        "batch_size": 1,
        "data_root": "/content/drive/My Drive/KO/",
        "train_datapath": "1-2020",
        "val_datapath": "",
        "test_datapath": "12-2020",
        "num_classes": 2,
        "filter_long_text": True
    },
    "model": "gpt2",
    "tokenizer": "gpt2",
    "model_param": {
        "vocab_size": 50257,
        "n_positions": 1024,
        "n_embd": 768,
        "n_layer": 12,
        "n_head": 12,
        "n_inner": None,
        "activation_function": "gelu_new",
        "resid_pdrop": 0.1,
        "embd_pdrop": 0.1,
        "atten_pdrop": 0.1,
        "layer_norm_epsilon": 1.0e-5,
        "initializer_range": 0.02,
        "summary_type": "cls_index",
        "summary_use_proj": True,
        "summary_activation": None,
        "summary_proj_to_labels": True,
        "summary_first_dropout": 0.1,
        "scale_attn_weights": True,
        "use_cache": True,
        "bos_token_id": 50256,
        "eos_token_id": 50256,
        "scale_attn_by_inverse_layer_idx": False,
        "reorder_and_upcast_attn": False
    },
    "trainer_param": {
        "epochs": 10,
        "val_epochs": 1,
        "loss_func": "cross_entropy",
        "metric": "acc",
        "optimizer": "AdamW",
        "optimizer_param": {
            "lr": 1.0e-5,
            "eps": 1.0e-6,
            "weight_decay": 0.0005
        }
    }
}